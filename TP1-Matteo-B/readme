You Can find all the source code and the makefile in this directory.

Unfortunatly I haven't been able to run this on the baobab cluster when I tried to use the E5-2630V4 (the jobs are still pending), and when I didn't specify any constraint I got ton of the following errors :

--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca mpi_cuda_support 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
...
/home/besanco4/tp1-t/TP1/./mpi: symbol lookup error: /opt/ebsofts/Compiler/GCC-CUDA/7.3.0-2.30-9.2.88/OpenMPI/3.1.1/lib/openmpi/mca_pml_ucx.so: undefined symbol: ucp_tag_send_nbr
...
srun: error: node076: tasks 1-10: Exited with exit code 127



Since I haven't been able to come to the last seminar I think I might have missed something..

after somme thinking I think that the following complexity might be:

MPI_Broadcast : O(1)
Simple broadcast : O(n^2)
Ring broadcast : O(n^2)
Hypercube : O(n(log2(n)))

It seems that the mpi_send is has a complexity of O(n), which helped me calculate those (https://stackoverflow.com/questions/10625643/mpi-communication-complexity).

The data I was able to gather on my local computer seems to confirm that. But since It is only able to run it with up to 8 thread.

Matteo Besan√ßon
